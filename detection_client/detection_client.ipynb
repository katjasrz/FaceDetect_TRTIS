{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-member",
   "metadata": {},
   "source": [
    "# Face detection application using the FaceDetect purpose built model from NVIDIA GPU Cloud (NGC): client app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-poster",
   "metadata": {},
   "source": [
    "Load the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-calcium",
   "metadata": {},
   "source": [
    "## Capturing a test image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-landing",
   "metadata": {},
   "source": [
    "Provide an image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"capture.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-turkey",
   "metadata": {},
   "source": [
    "Get an image from a webcam. A window with camera feed will appear, press 's' button on your keyboard in order to capture an image of yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    return_value, capture = camera.read()\n",
    "    cv2.imshow('capture', capture)\n",
    "    if cv2.waitKey(1)& 0xFF == ord('s'):\n",
    "        cv2.imwrite(image_name, capture)\n",
    "        break\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-logic",
   "metadata": {},
   "source": [
    "A function to display your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img):\n",
    "    dpi = 80\n",
    "    height, width, depth = img.shape\n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(capture)\n",
    "print(capture.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-scout",
   "metadata": {},
   "source": [
    "If you do not have a web camera, you can take any JPG image of a person, put it to the `client` directory of this project and rename it as `capture.jpg`. Note, that the preprocessing function of this demo has been designed assuming that image width is larger than image height (works best with horizontal images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-encyclopedia",
   "metadata": {},
   "source": [
    "## Image pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-herald",
   "metadata": {},
   "source": [
    "The following code will load and modify your original image to make it compatible with the network. FaceDetect model expects images with width = 736 and height = 416. This information is available in the corresponding [model card](https://ngc.nvidia.com/catalog/models/nvidia:tlt_facenet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_resize_and_pad(image, w, h):\n",
    "    # this example assumes that image width > image height\n",
    "\n",
    "    ratio = h/image.size[1]\n",
    "    image_resized = image.resize((int(image.size[0]*ratio),h), Image.BILINEAR)\n",
    "    \n",
    "    new_im = Image.new(\"RGB\", (w, h))\n",
    "    temp_width_cache = image_resized.size[0] # will be useful in the end to restore the original size\n",
    "\n",
    "    # if the new width is larger than target w it will be cropped, otherwise padded\n",
    "    new_im.paste(image_resized, ((w - image_resized.size[0])//2,0))    \n",
    "\n",
    "    return new_im, ratio, temp_width_cache\n",
    "\n",
    "\n",
    "def process_image(image_name, w, h):\n",
    "    img = Image.open(image_name)\n",
    "    # resize image keeping proportions\n",
    "    image_resized, ratio, temp_width_cache = im_resize_and_pad(img, w, h)\n",
    "    im = np.array(image_resized, dtype=\"float32\")\n",
    "    im = np.rollaxis(im, axis=2)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    # Normalize to [0.0, 1.0] interval (expected by model)\n",
    "    im = (1.0 / 255.0) * im\n",
    "    return im, ratio, temp_width_cache\n",
    "\n",
    "\n",
    "model_h = 416\n",
    "model_w = 736\n",
    "\n",
    "im, ratio, temp_width_cache = process_image(image_name, model_w, model_h)\n",
    "\n",
    "print(im.shape)\n",
    "print(ratio)\n",
    "print(temp_width_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-strategy",
   "metadata": {},
   "source": [
    "Batched input (we will be using the same image to create a batch just for the purpose of performance demonstration). Note, that the bacth size value should not be bigger than the max value you specified in parameter `-m` when converting the model with `tlt-converter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "im_batch = np.concatenate([im[:]]*batch_size)\n",
    "print(im_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-cricket",
   "metadata": {},
   "source": [
    "## Executing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tritonclient.grpc as tritongrpcclient\n",
    "\n",
    "url = 'localhost:8001'\n",
    "model_name = 'facedetect_tlt'\n",
    "\n",
    "try:\n",
    "    triton_client = tritongrpcclient.InferenceServerClient(\n",
    "        url=url,\n",
    "        verbose=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))\n",
    "    sys.exit()\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "inputs.append(tritongrpcclient.InferInput('input_1', im_batch.shape, \"FP32\"))\n",
    "inputs[0].set_data_from_numpy(im_batch)\n",
    "\n",
    "outputs.append(tritongrpcclient.InferRequestedOutput('output_bbox/BiasAdd'))\n",
    "outputs.append(tritongrpcclient.InferRequestedOutput('output_cov/Sigmoid'))\n",
    "\n",
    "results = triton_client.infer(model_name=model_name,\n",
    "                              inputs=inputs,\n",
    "                              outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-pastor",
   "metadata": {},
   "source": [
    "## Parsing outputs and post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-rapid",
   "metadata": {},
   "source": [
    "Extract results as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = results.as_numpy('output_bbox/BiasAdd')\n",
    "print(bboxes.shape)\n",
    "scores = results.as_numpy('output_cov/Sigmoid')\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-juice",
   "metadata": {},
   "source": [
    "Set model related patameters. Box scale and offset are are parameters which were used to train the model with Transfer Learning Toolkit. These values are also listed in the [model card](https://ngc.nvidia.com/catalog/models/nvidia:tlt_facenet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_scale = 35.0   \n",
    "box_offset = 0.5  \n",
    "\n",
    "grid_h = 26\n",
    "grid_w = 46\n",
    "\n",
    "# Threshold for detection score\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-impact",
   "metadata": {},
   "source": [
    "Parse the list of detected bounding boxes and translate the values into the coordinate system of the pre-processed image. Note, that we are not going through the whole batch, since we just used multiple copies of the same image. You can set any value of `batch_idx` from the interval `[0, batch_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_width  = model_w  / grid_w\n",
    "cell_height = model_h / grid_h\n",
    "\n",
    "bboxes_list = []\n",
    "scores_list = []\n",
    "\n",
    "batch_idx = 15\n",
    "\n",
    "for h in range (grid_h):\n",
    "    for w in range (grid_w):\n",
    "        score = scores[batch_idx, 0, h, w]\n",
    "        if (score > threshold):\n",
    "            \n",
    "            # location of the w, h coordinate in the original image\n",
    "            mx = w * cell_width;\n",
    "            my = h * cell_height;\n",
    "            \n",
    "            # scale the detected coordinates to original and return their location in the image\n",
    "            rectX1f = - (bboxes[batch_idx, 0, h, w] + box_offset) * box_scale + mx\n",
    "            rectY1f = - (bboxes[batch_idx, 1, h, w] + box_offset) * box_scale + my\n",
    "            rectX2f = (bboxes[batch_idx, 2, h, w] + box_offset) * box_scale + mx\n",
    "            rectY2f = (bboxes[batch_idx, 3, h, w] + box_offset) * box_scale + my\n",
    "\n",
    "            xmin = int(rectX1f)\n",
    "            ymin = int(rectY1f)\n",
    "            xmax = int(rectX2f)\n",
    "            ymax = int(rectY2f)\n",
    "\n",
    "            bboxes_list.append([xmin, ymin, xmax, ymax])\n",
    "            scores_list.append(float(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-saudi",
   "metadata": {},
   "source": [
    "Apply non-max suppression to the list of all detected bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = cv2.dnn.NMSBoxes(bboxes_list, scores_list, threshold, 0.5)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-prime",
   "metadata": {},
   "source": [
    "Show the detected bounding box in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(image_name)\n",
    "for idx in indexes:\n",
    "    idx = int(idx)\n",
    "    xmin, ymin, xmax, ymax = bboxes_list[idx]\n",
    "    xmin = int((xmin - (model_w - temp_width_cache)/2)/ratio)\n",
    "    xmax = int((xmax - (model_w - temp_width_cache)/2)/ratio)\n",
    "    ymin = int(ymin / ratio)\n",
    "    ymax = int(ymax / ratio)\n",
    "    color = [0, 255, 0]\n",
    "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "\n",
    "display_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-least",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
